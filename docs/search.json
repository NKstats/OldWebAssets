[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "NKstats",
    "section": "",
    "text": "Welcome to the NKstats Blog! Explore my latest posts below.\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nText Preprocessing for Sentiment Analysis\n\n\n\nProgramming\n\n\n\nA guide to pre-processing text data for sentiment analysis in Python.\n\n\n\nSolomon Eshun\n\n\nApr 11, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Readme.html",
    "href": "Readme.html",
    "title": "<<<<<<< HEAD",
    "section": "",
    "text": "&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n\n\n\n\n\n\n\n6e566f9dd1699045ba8fe7ac06b9bb05071f6302"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "NKstats",
    "section": "",
    "text": "Contact\n\n\n\n\n\n\n\n\nSend"
  },
  {
    "objectID": "blogs/sentiment.html#outline",
    "href": "blogs/sentiment.html#outline",
    "title": "Text Preprocessing for Sentiment Analysis",
    "section": "Outline",
    "text": "Outline\n\nIntroduction & Method\nPython Implementation"
  },
  {
    "objectID": "blogs/sentiment.html#Introduction-1",
    "href": "blogs/sentiment.html#Introduction-1",
    "title": "Text Preprocessing for Sentiment Analysis",
    "section": "Introduction & Method",
    "text": "Introduction & Method\nSentiment analysis is a critical component of natural language processing (NLP) that helps decipher the emotional tone behind words. This analysis is particularly valuable not only in social media monitoring and market research but also in public health initiatives. Understanding public sentiment can significantly enhance health communication strategies, patient engagement, and policy formulation. Moreover, in public health crises, sentiment analysis can help track community reactions to health advisories or public sentiment toward vaccination drives, thus informing more targeted health communication and intervention strategies. However, the effectiveness of sentiment analysis in these sensitive areas is heavily reliant on the quality of preprocessing applied to text data. This post details my approach to preprocessing texts to enhance sentiment analysis. Through a series of carefully designed steps, I aim to transform raw text data into a clean, analyzable format, setting the stage for accurate classification.\n\n\n\nHere’s a detailed breakdown of the preprocessing techniques I employed to refine the raw tweet data:\n\nURL Removal: I began by stripping URLs from the texts. URLs usually don’t contribute to sentiment and can add irrelevant noise to the text, so removing them helps in focusing on meaningful content.\nTokenization: Next, I tokenized the text by splitting the texts into individual words. This is crucial as it allows us to process the text at the word level, which is essential for the subsequent cleaning and analysis steps.\nRemoving Stopwords: I then removed stopwords - common words such as “the,” “and,” “is,” etc., which are abundant in English but do not carry significant sentiment. Eliminating these words helps reduce textual noise and focuses the analysis on more impactful words that convey sentiment.\nEliminating Special Characters: I also scrubbed the text of special characters like punctuation marks, symbols and emojis. These characters do not typically contribute to sentiment and can impede the performance of sentiment analysis models.\nLemmatization: Finally, I applied lemmatization to consolidate similar words to their base or dictionary forms. Unlike stemming, which simply chops off the ends of words to reach a common base, lemmatization considers the context and morphological analysis of words to accurately transform them to their lemma. This process reduces the number of unique words the model must handle, enhancing its ability to generalize across different variations of the same word.\n\nThrough these text preprocessing steps, I transformed the raw text data into a clean and standardized format. This preparation is vital for conducting accurate sentiment analysis as it helps eliminate noise, reduce the complexity of the data, and focus on the most impactful elements of the text. The preprocessed texts are now ready for further analysis and can be effectively used in sentiment classification models to understand public sentiment."
  },
  {
    "objectID": "blogs/sentiment.html#Python-codes-implementation",
    "href": "blogs/sentiment.html#Python-codes-implementation",
    "title": "Text Preprocessing for Sentiment Analysis",
    "section": "Python Implementation",
    "text": "Python Implementation\nBefore creating the preprocessing function, it’s essential to set up the environment with the required packages. This includes loading libraries and downloading necessary data for text manipulation.\n\n# Install the emoji package before loading\n!pip install emoji\n\n\n# Load Required packages\nimport pandas as pd\npd.set_option('display.max_colwidth', None)\nimport numpy as np\nimport nltk, re, emoji, spacy\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n# Download necessary NLTK resources\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n\n\n\n\nThe preprocess_text function below incorporates all the preprocessing steps discussed in the Introduction & Method section. It is structured to be reusable for any text dataset that requires cleaning for sentiment analysis.\n\n# Load the spaCy model\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef preprocess_text(data):\n    processed_data = []\n    # Set of English stopwords\n    stop = set(stopwords.words('english'))\n    for sentence in data:\n        # Remove URLs\n        text = re.sub(r'https?://\\S+|www\\.\\S+', '', sentence)\n\n        # Remove emojis using the emoji library\n        text = emoji.replace_emoji(text, replace='')\n\n        # Create spaCy doc object\n        doc = nlp(text)\n\n        lem_list = []\n        for token in doc:\n            if token.is_alpha and token.text.lower() not in stop:  # Check if token is alphabetic and not a stopword\n                if token.pos_ != \"PRON\":  # Exclude all pronouns\n                    lem_list.append(token.lemma_.lower())\n\n        # Rejoin words to form the final cleaned text\n        final_text = ' '.join(lem_list)\n        processed_data.append(final_text)\n\n    return processed_data\n\n\n\n\nTo demonstrate the functionality, the preprocessing function is applied to a sample dataset containing reviews. This will output the cleaned versions of the input reviews, showing how punctuation, common words, and case have been normalized.\n\n# Sample input text\nsample_text = [\"The pandemic has affected the world.. @100% https://example.com!! :)\"]\ncleaned_text = preprocess_text(sample_text)\ncleaned_text\n\n['pandemic affect world']\n\n\n\n\n\nNext, I create a DataFrame with 20 sample reviews, ratings, and IDs, and apply the function to clean the review texts.\n\n# Creating a sample DataFrame with texts\n\ndata = {\n    'Review': [\n        \"Lockdown was necessary for our safety, I fully support it! 😊\",\n        \"I understand the need but it was too long. 😕\",\n        \"Lockdown completely disrupted my life, it was terrible. 😡\",\n        \"It was well-handled and necessary for public health. 🏥\",\n        \"Lockdown helped us control the virus spread. 😷\",\n        \"It was too restrictive and unnecessary for so long. 😒\",\n        \"Not sure if lockdown was the best solution, but we had no choice. 🤷\",\n        \"It saved lives, but at a great personal cost to many. 😔\",\n        \"The government did what was necessary during the lockdown. 👍\",\n        \"Lockdown was too harsh and not managed well. 😠\",\n        \"It was a good measure, but I hated being stuck at home. 🏠😖\",\n        \"Lockdown was essential but mental health suffered a lot. 🧠💔\",\n        \"The benefits outweigh the negatives of the lockdown. ➕➖\",\n        \"Poor execution made it harder than it should be. 😞\",\n        \"Lockdown was over before it was truly safe to do so. 😨\",\n        \"We needed the lockdown to protect vulnerable populations. 🛡️\",\n        \"The economic impact of lockdown was devastating. 💸\",\n        \"Lockdown showed we can take collective action when needed. 🤝\",\n        \"It was necessary, but the government support was insufficient. 💔\",\n        \"Lockdown should have been stricter to be more effective. ⚠️\"\n    ],\n    'Sentiment_Rating': [5, 3, 1, 4, 5, 2, 3, 3, 4, 1, 3, 2, 4, 2, 3, 5, 1, 4, 2, 4]\n}\n\n# Creating the DataFrame\nreviews_df = pd.DataFrame(data)\n\n# Applying the text preprocessing function\nreviews_df['Cleaned_Review'] = preprocess_text(reviews_df['Review'].tolist())\nreviews_df\n\n\n\n\n\n\n\n\n\nReview\nSentiment_Rating\nCleaned_Review\n\n\n\n\n0\nLockdown was necessary for our safety, I fully support it! 😊\n5\nlockdown necessary safety fully support\n\n\n1\nI understand the need but it was too long. 😕\n3\nunderstand need long\n\n\n2\nLockdown completely disrupted my life, it was terrible. 😡\n1\nlockdown completely disrupt life terrible\n\n\n3\nIt was well-handled and necessary for public health. 🏥\n4\nwell handle necessary public health\n\n\n4\nLockdown helped us control the virus spread. 😷\n5\nlockdown help control virus spread\n\n\n5\nIt was too restrictive and unnecessary for so long. 😒\n2\nrestrictive unnecessary long\n\n\n6\nNot sure if lockdown was the best solution, but we had no choice. 🤷\n3\nsure lockdown good solution choice\n\n\n7\nIt saved lives, but at a great personal cost to many. 😔\n3\nsave life great personal cost many\n\n\n8\nThe government did what was necessary during the lockdown. 👍\n4\ngovernment necessary lockdown\n\n\n9\nLockdown was too harsh and not managed well. 😠\n1\nlockdown harsh manage well\n\n\n10\nIt was a good measure, but I hated being stuck at home. 🏠😖\n3\ngood measure hate stick home\n\n\n11\nLockdown was essential but mental health suffered a lot. 🧠💔\n2\nlockdown essential mental health suffer lot\n\n\n12\nThe benefits outweigh the negatives of the lockdown. ➕➖\n4\nbenefit outweigh negative lockdown\n\n\n13\nPoor execution made it harder than it should be. 😞\n2\npoor execution make hard\n\n\n14\nLockdown was over before it was truly safe to do so. 😨\n3\nlockdown truly safe\n\n\n15\nWe needed the lockdown to protect vulnerable populations. 🛡️\n5\nneed lockdown protect vulnerable population\n\n\n16\nThe economic impact of lockdown was devastating. 💸\n1\neconomic impact lockdown devastating\n\n\n17\nLockdown showed we can take collective action when needed. 🤝\n4\nlockdown show take collective action need\n\n\n18\nIt was necessary, but the government support was insufficient. 💔\n2\nnecessary government support insufficient\n\n\n19\nLockdown should have been stricter to be more effective. ⚠️\n4\nlockdown strict effective"
  },
  {
    "objectID": "blogs/ex1.html",
    "href": "blogs/ex1.html",
    "title": "Propensity Score Matching in Python",
    "section": "",
    "text": "under construction !!!"
  },
  {
    "objectID": "blogs/ex1.html#introduction",
    "href": "blogs/ex1.html#introduction",
    "title": "Propensity Score Matching in Python",
    "section": "Introduction",
    "text": "Introduction\nIn statistical analysis, the challenge of comparing treatments in observational studies is significant due to the presence of confounding variables. Unlike randomized controlled trials (RCTs), observational studies do not randomly assign treatments, which can lead to biased estimates of treatment effects. This is where the idea of propensity score matching (PSM) becomes an invaluable tool.\nPSM is a statistical technique that attempts to estimate the effect of a treatment or intervention by accounting for the covariates that predict receiving the treatment (or intervention). It was developed by Rosenbaum and Rubin (1983) to help reduce bias by matching units receiving the treatment with similar units that do not receive the treatment based on propensity scores. This score is the probability of treatment assignment conditional on observed baseline characteristics. PSM helps to form a sample in which the distribution of observed baseline covariates is similar between treated and untreated groups, thereby mimicking a randomized experiment.\nIn this blog post, I will explore the implementation of PSM in Python using the Cleveland heart disease datasets from the UCI repository (Janosi and Detrano 1988). This guide will detail the necessary packages, provide step-by-step coding examples, and offer practical advice to optimize your analysis. This post is designed for researchers, statisticians, and data enthusiasts who want to improve their skills in using PSM with observational data, especially when employing Python for their analyses.\nLet’s get started by exploring and preparing the dataset."
  },
  {
    "objectID": "blogs/ex1.html#dataset-variable-descriptions",
    "href": "blogs/ex1.html#dataset-variable-descriptions",
    "title": "Propensity Score Matching in Python",
    "section": "Dataset Variable Descriptions",
    "text": "Dataset Variable Descriptions\n\n\nDataset Variable Descriptions\n\n\nage: Age of the patient.\nsex: Sex of the patient (1 = male; 0 = female).\ncp: Chest pain type with the following categories:\n\n1: typical angina\n2: atypical angina\n3: non-anginal pain\n4: asymptomatic\n\ntrestbps: Resting blood pressure (in mm Hg on admission to the hospital).\nchol: Serum cholesterol in mg/dl.\nfbs: Fasting blood sugar &gt; 120 mg/dl (1 = true; 0 = false).\nrestecg: Resting electrocardiographic results (Values 0, 1, 2).\nthalach: Maximum heart rate achieved.\nexang: Exercise-induced angina (1 = yes; 0 = no).\noldpeak: ST depression induced by exercise relative to rest.\nslope: The slope of the peak exercise ST segment.\nca: Number of major vessels (0-3) colored by fluoroscopy.\nthal: A short form for thalassemia (3 = normal; 6 = fixed defect; 7 = reversible defect).\nnum (the predicted attribute): Diagnosis of heart disease (angiographic disease status) —\n\nValue 0: &lt; 50% diameter narrowing\nValues 1-4: &gt; 50% diameter narrowing\n\nexang (Exercise Induced Angina): This variable is binary (1 = yes, 0 = no) and indicates whether the patient experienced angina (chest pain) induced by exercise. It is a good candidate for a treatment variable because angina can be considered as an “event” or “condition” whose impact on other health outcomes can be assessed.\nnum (Diagnosis of Heart Disease): This is the outcome variable and is already coded as 0 = no presence and 1-4 indicating different types of defects (often, any value &gt; 0 is considered as presence of disease). This variable can be used to determine the effect of exercise-induced angina on the presence of heart disease.\n\n\n\nimport pandas as pd\nimport numpy as np\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n# Column names based on the dataset documentation\ncolumn_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n                'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n\n# Load the dataset\ndf = pd.read_csv(url, names=column_names)\n\n# Replace '?' with NaN\ndf.replace('?', np.nan, inplace=True)\ndf.dropna(inplace=True) # drop rows with NaN\n\n\ndf['num'] = (df['num'] &gt; 0).astype(int)\n\ncontinuous_vars = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n\n# Converting all other variables to 'category' data type except the ones in continuous_vars\nfor column in df.columns:\n    if column not in continuous_vars:\n        # First converting to float to handle any decimal string representations\n        df[column] = df[column].astype('float').astype('int64').astype('category')\n\ndf = pd.get_dummies(df, columns=['thal', 'cp', 'restecg'])\ndf = df.drop(columns=['thal_6', 'restecg_1', 'cp_1'])\n\n# List of columns that contain boolean values\nbool_columns = ['thal_3', 'thal_7', 'cp_2', 'cp_3', 'cp_4', 'restecg_0', 'restecg_2']\n\n# Converting boolean values to integers (1 for True, 0 for False)\nfor column in bool_columns:\n    df[column] = df[column].astype(int).astype('category')\n\n\n\nDistribution of the Variables with respect to Exang\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Filtering out the 'num' and 'exang' columns from plotting\nplot_columns = df.columns[~df.columns.isin(['num', 'exang'])]\n\n# Determine the number of subplots\nn_vars = plot_columns.size\nn_cols = 4\nn_rows = (n_vars + n_cols - 1) // n_cols  # This ensures enough rows to accommodate all plots\n\n# Create a figure to hold the subplots\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\naxes = axes.flatten()  # Flatten the axes array for easy iteration\n\n# Iterate through the list of columns and create plots\nfor i, column in enumerate(plot_columns):\n    ax = axes[i]\n    if df[column].dtype in ['int64', 'float64']:  # Adjust based on your specific data types\n        sns.histplot(data=df, x=column, hue='exang', element='step', stat='density', common_norm=False, ax=ax)\n        ax.set_title(f'Distribution of {column} by Exang')\n        ax.set_xlabel(column)\n        ax.set_ylabel('Density')\n        ax.legend(title='Exang', labels=['No Angina (0)', 'With Angina (1)'], loc='best')\n    else:\n        sns.countplot(x=column, hue='exang', data=df, ax=ax)\n        ax.set_title(f'Count of {column} by Exang')\n        ax.set_xlabel(column)\n        ax.set_ylabel('Count')\n        ax.legend(title='Exang', labels=['No Angina (0)', 'With Angina (1)'], loc='best')\n\n# Adjust layout to prevent overlap and ensure labels are readable\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blogs/ex1.html#propensity-score-estimation",
    "href": "blogs/ex1.html#propensity-score-estimation",
    "title": "Propensity Score Matching in Python",
    "section": "Propensity Score Estimation",
    "text": "Propensity Score Estimation\nIn the PSM implementation, I will use the psmpy package developed by Kline and Luo (2022)\n\nfrom psmpy import PsmPy\nfrom psmpy.plotting import *\nimport warnings\nwarnings.filterwarnings('ignore')\nimport math\n\n\ncovariate = 'age'\n\ndata = df\n\npepe_calories = data[data['exang'] == 1][covariate]\nmodern_calories = data[data['exang'] == 0][covariate]\nplt.figure(figsize=(8,4))\nax = sns.distplot(pepe_calories, fit_kws={\"color\":\"'orange'\"}, kde=True,#palette=[\"#3498db\",'orange']\n        hist=None, label=\"exang = 0\", kde_kws={\"linewidth\": 2.5});\nax = sns.distplot(modern_calories, fit_kws={\"color\":\"#3498db\"}, kde=True,\n        hist=None, label=\"exang = 1\", kde_kws={\"linewidth\": 2.5});\n\nplt.xlabel('Propensity Score')\nplt.legend(fontsize=10)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.xlabel('Estimated Ps', fontsize=10, labelpad=10)\nplt.ylabel(' ', fontsize=10, labelpad=10)\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nplt.yticks([])\nplt.title('Before Matching', fontsize=10)\nplt.show()\n\n\n\n\n\n\n\n\n\nThesis\nCausal inference is a vital aspect of medical research that helps to establish causal relationships between different variables. It is the process of making claims about cause-and-effect relationships in a given system or phenomenon. It is an essential component of scientific research, particularly in fields such as medicine, economics, and social sciences, where experiments that manipulate variables are often not feasible or ethical. In these cases, researchers often rely on comparative experimental studies.\nObservational studies become important in fields such as medicine, economics, and social sciences where it may not be feasible or ethical to conduct experiments that manipulate variables. However, making causal inferences in observational studies can be challenging due to limitations such as random selection of subjects but not random allocation of treatments to subjects. This makes it difficult to determine whether the difference in outcome between treated and untreated subjects is due to the treatment or differences in other characteristics of the subjects.\nAnother limitation is self-selection, where individuals opt for a particular treatment for specific reasons, making it difficult to compare them directly with those who did not receive the treatment.\nThere is an increasing interest in estimating the causal effects of treatment using observational or nonrandomized data. In observational studies, the baseline characteristics of treated or exposed subjects often differ systematically from those of untreated or unexposed subjects. Essential to the production of high-quality evidence to inform decision-making is the ability to minimize the effect of confounding. An increasingly frequent approach to minimizing bias when estimating causal treatment effects is based on the propensity score (the likelihood of receiving treatment based on some outlines subject characteristics) usually obtained using a logit model Rosenbaum and Rubin (1983).\nPropensity score matching have gained much relevance across disciplines to estimate causal effects using observational data. This technique attempt to replicate the ideal of randomized experiments as closely as possible. It a quasi-experimental method that aims to search for counterfactual unit that is comparable with the treated unit among many untreated units."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "NKstats",
    "section": "",
    "text": "Publication\n\n\n\n\nS. Eshun. Equity in Focus: Investigating Gender Disparities in Glioblastoma via Propensity Score Matching. JMIRx Med (In Review: since August 15, 2023).\nS. Eshun, R. Essieku, J. Ladzekpo (2023). Stability Analyses on the Effect of Vaccination and Contact Tracing in Monkeypox Virus Transmission. Journal of Mathematical & Computational Science, 13. Paper Link\nD. Florez, E. Romero-Severson, K.M. Martinez, C. Franco, S. Eshun, J.A. Spencer, C. Manore, M. Mancuso, J. Keithley (2022). Assessing the Risk of Dengue Outbreaks across Continental Biomes in Brazil. In AGU Fall Meeting Abstracts. 2022, GH25D-0625. Abstract Link\n\n\n\n\n \n\n\n\n\n\nWorking Papers\n\n\n\n\nS. Eshun, T. Oraby, A. Al-Mamun, T. Brothers, Z. Mohamed. Assessing the Effect of Sodium-Glucose Cotransporter-2 Inhibitors on Chronic Kidney Disease in Diabetes Patients: A Causal Machine Learning Approach.\nMd S. Mahmud, S. Eshun, C. Kadelka, B. Espinoza. An Exploration of Population-Wide Behavioral Fear Responses that Yield Epidemic Waves."
  },
  {
    "objectID": "blogs/ex3.html",
    "href": "blogs/ex3.html",
    "title": "Still Working2!",
    "section": "",
    "text": "under construction !"
  },
  {
    "objectID": "conference.html",
    "href": "conference.html",
    "title": "NKstats",
    "section": "",
    "text": "Conference Presentations\n\n\n\n\n“Estimating Gender Differences in Glioblastoma Outcomes using Propensity Score Matching” (Oral), 2nd Interdisciplinary Biological Sciences Symposium, Ames, IA, 04/03/2024\n“Investigating Gender Disparities in Glioblastoma via Propensity Score Matching” (Lightning talk & Poster), Boston Pharmaceutical Symposium, Sanofi, Cambridge, MA, 10/13/2023.\n“Equity in Focus: Investigating Gender Disparities in Glioblastoma via Propensity Score Matching” (Poster), Statistics in Pharmaceuticals Conference, University of Connecticut (Virtual), 08/17/2023.\nS. Eshun, T. Oraby, & M. Al-Mamun., “A Machine Learning Approach to Evaluate the Effect of SGLT2 on CKD in Diabetes Patients” (Poster), College of Sciences Annual Research Conference, Brownsville, TX, 04/28/2023.\n“Diagnosing CKD in Type 2 Diabetes Patients: A Machine Learning Approach” (Poster), 9th Annual TRACS Summit, Texas A&M, College Station, TX, 04/03/2023.\n“Diagnosing CKD & Heart Failure in Type 2 Diabetes Patients: A Machine Learning Approach” (Poster), School of Interdisciplinary Programs & Community Engagement (SIPCE) 2nd Annual Conference, Edinburg, TX, 03/30/2023."
  },
  {
    "objectID": "NKstats.html",
    "href": "NKstats.html",
    "title": "NKstats",
    "section": "",
    "text": "Hi! My name is Solomon Eshun (he/him/his). I am a PhD student in Applied Mathematics at Iowa State University.\nMy goal is to master statistical methodologies that can identify the most effective treatments and interventions by analyzing data from observational studies, clinical trials, etc. I am particularly interested in combining machine learning with causal methods to analyze the effects of policies and interventions on disease outcomes, addressing the challenges posed by heterogeneous patient characteristics and varying medical protocols within healthcare datasets. Additionally, I am interested in exploring the dynamics of infectious disease transmission by developing mathematical and statistical models to simulate disease spread and evaluate the effectiveness of control strategies.\n\n\n\n\nExpand All Collapse All\n\n\n\n\n\n\n    Research Interests\n    \n    Causal Inference, Precision Medicine, Machine Learning, Infectious Disease Modeling\n\n\n\n\n\n\nEducation\n\n\n    \n        PhD (Student) in Applied Mathematics\n        Iowa State University, Ames, 2023 - 2024\n     \n    \n        MS in Applied Statistics & Data Science\n        University of Texas Rio Grande Valley (UTRGV), Edinburg, 2023\n    \n    \n        BS in Mathematics\n        University of Mines and Technology (UMaT), Ghana, 2020\n    \n\n\n\n\n\n\n\nExperience\n\n\n    \n        Graduate Research Assistant, .......... (08/2024 - Present)\n     \n    \n        Graduate Fellow, Data Science for the Public Good, Iowa State (05/2024 - 07/2024)\n    \n    \n        Graduate Teaching Assistant, Department of Mathematics, Iowa State (08/2023 - 05/2023)\n    \n    \n    \n        Sustainability Research Fellow, Office for Sustainability, UTRGV (08/2022 - 05/2023)\n        \n    \n        Graduate Teaching Assistant, School of Mathematical & Statistical Sciences, UTRGV (08/2021 - 05/2023)\n        \n    \n        Computational Science Intern, Los Alamos National Lab, Los Alamos, NM (05/2022 - 08/2022)\n        \n    \n        Teaching Assistant, Department of Mathematical Sciences, UMaT (09/2020 - 08/2021)\n    \n        \n    \n        Data Production Intern, Ghana Statistical Service, Accra (05/2019 - 08/2019)\n    \n\n\n\n\n\n\nHonors & Awards\n\n\n    \n        The Cole, 1958 & Green Scholarships, 2024.\n    \n    \n    \n        Travel Award, Department of Mathematics, Iowa State University, 2023.\n    \n    \n    \n        Certificate of Excellence in Sustainable Development Graduate Research, UTRGV, 2023.\n    \n    \n    \n        UTRGV Applied Statistics & Data Science Outstanding M.S. Thesis Award, 2023.\n    \n    \n    \n        Poster Award, SIPCE 2nd Annual Conference for Interdisciplinary Research, UTRGV, 2023.\n    \n    \n    \n        Sustainability Research Fellowship, Office for Sustainability, UTRGV, 2022.\n    \n    \n    \n        Burkhart Endowed Scholarship, School of Mathematical & Statistical Sciences, UTRGV, 2022.\n    \n    \n    \n        Joseph Wiener Math Scholarship, School of Mathematical & Statistical Sciences, UTRGV, 2022.\n    \n    \n    \n        Vice Chancellor's Overall Best Graduating Student, University of Mines & Technology, 2020.\n    \n    \n    \n        Best Graduating Student, Faculty of Engineering, University of Mines & Technology, 2020.\n    \n    \n    \n        Best Student, Department of Mathematical Sciences, University of Mines & Technology, 2020."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "NKstats",
    "section": "",
    "text": "Welcome to NKstats! This page provides information about my projects, goals, and background.\nThe full site is under construction …"
  }
]