[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "NKstats",
    "section": "",
    "text": "Welcome to the NKstats Blog! Explore our latest posts below.\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nStill Working2!\n\n\n\nstatistics\n\n\nprogramming\n\n\n\n\n\n\n\nSolomon Eshun\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity Score Matching in Python\n\n\n\nprogramming\n\n\n\nA step-by-step approach to conducting propensity score matching in Python\n\n\n\nSolomon Eshun\n\n\nApr 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning Text Data in Python\n\n\n\nprogramming\n\n\n\nA guide to pre-processing text data for sentiment analysis in Python.\n\n\n\nSolomon Eshun\n\n\nApr 11, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Readme.html",
    "href": "Readme.html",
    "title": "<<<<<<< HEAD",
    "section": "",
    "text": "&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n\n\n\n\n\n\n\n6e566f9dd1699045ba8fe7ac06b9bb05071f6302"
  },
  {
    "objectID": "write_ups/ex3.html",
    "href": "write_ups/ex3.html",
    "title": "Still Working2!",
    "section": "",
    "text": "under construction !"
  },
  {
    "objectID": "NKstats.html",
    "href": "NKstats.html",
    "title": "NKstats",
    "section": "",
    "text": "Hi! My name is Solomon Eshun (he/him/his). I am a PhD student in Applied Mathematics at Iowa State University.\nI am dedicated to developing statistical methods and tools that address public health challenges. My goal is to master methodologies that can identify the most effective treatments and interventions by analyzing data from observational studies, clinical trials and other sources. I am particularly interested in combining machine learning with causal methods to analyze the effects of policies and interventions on disease outcomes, addressing the challenges posed by heterogeneous patient characteristics and varying medical protocols within healthcare datasets. Additionally, I am interested in exploring the dynamics of infectious disease transmission by developing mathematical and statistical models to simulate disease spread and evaluate the effectiveness of control strategies.\n\n\n\n\n\n\n\n\nExpand All Collapse All\n\n\n\n    Research Interests\n    \n    Causal Inference, Precision Medicine, Machine Learning, Infectious Disease Modeling\n\n\n\n\n\n\nEducation\n\n\n    \n        PhD (Student) in Applied Mathematics\n        Iowa State University, Ames, 2023 - 2024\n     \n    \n        MS in Applied Statistics & Data Science\n        University of Texas Rio Grande Valley (UTRGV), Edinburg, 2023\n    \n    \n        BS in Mathematics\n        University of Mines and Technology (UMaT), Ghana, 2020\n    \n\n\n\n\n\n\n\nExperience\n\n\n    \n        Graduate Research Assistant, .......... (08/2024 - Present)\n     \n    \n        Graduate Fellow, Data Science for the Public Good, Iowa State (05/2024 - 07/2024)\n    \n    \n        Graduate Teaching Assistant, Department of Mathematics, Iowa State (08/2023 - 05/2023)\n    \n    \n    \n        Sustainability Research Fellow, Office for Sustainability, UTRGV (08/2022 - 05/2023)\n        \n    \n        Graduate Teaching Assistant, School of Mathematical & Statistical Sciences, UTRGV (08/2021 - 05/2023)\n        \n    \n        Computational Science Intern, Los Alamos National Lab, Los Alamos, NM (05/2022 - 08/2022)\n        \n    \n        Teaching Assistant, Department of Mathematical Sciences, UMaT (09/2020 - 08/2021)\n    \n        \n    \n        Data Production Intern, Ghana Statistical Service, Accra (05/2019 - 08/2019)\n    \n\n\n\n\n\n\nHonors & Awards\n\n\n    \n        The Cole, 1958 & Green Scholarships, 2024.\n    \n    \n    \n        Travel Award, Department of Mathematics, Iowa State University, 2023.\n    \n    \n    \n        Certificate of Excellence in Sustainable Development Graduate Research, UTRGV, 2023.\n    \n    \n    \n        UTRGV Applied Statistics & Data Science Outstanding M.S. Thesis Award, 2023.\n    \n    \n    \n        Poster Award, SIPCE 2nd Annual Conference for Interdisciplinary Research, UTRGV, 2023.\n    \n    \n    \n        Sustainability Research Fellowship, Office for Sustainability, UTRGV, 2022.\n    \n    \n    \n        Burkhart Endowed Scholarship, School of Mathematical & Statistical Sciences, UTRGV, 2022.\n    \n    \n    \n        Joseph Wiener Math Scholarship, School of Mathematical & Statistical Sciences, UTRGV, 2022.\n    \n    \n    \n        Vice Chancellor's Overall Best Graduating Student, University of Mines & Technology, 2020.\n    \n    \n    \n        Best Graduating Student, Faculty of Engineering, University of Mines & Technology, 2020.\n    \n    \n    \n        Best Student, Department of Mathematical Sciences, University of Mines & Technology, 2020."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "NKstats",
    "section": "",
    "text": "Publications\n\n\n\n\nS. Eshun. Equity in Focus: Investigating Gender Disparities in Glioblastoma via Propensity Score Matching. JMIRx Med (In Review: since August 15, 2023).\nS. Eshun, R. Essieku, J. Ladzekpo (2023). Stability Analyses on the Effect of Vaccination and Contact Tracing in Monkeypox Virus Transmission. Journal of Mathematical & Computational Science, 13. Paper Link\nD. Florez, E. Romero-Severson, K.M. Martinez, C. Franco, S. Eshun, J.A. Spencer, C. Manore, M. Mancuso, J. Keithley (2022). Assessing the Risk of Dengue Outbreaks across Continental Biomes in Brazil. In AGU Fall Meeting Abstracts. 2022, GH25D-0625. Abstract Link"
  },
  {
    "objectID": "conference.html",
    "href": "conference.html",
    "title": "NKstats",
    "section": "",
    "text": "Conference Presentations\n\n\n\n\nâ€œEstimating Gender Differences in Glioblastoma Outcomes using Propensity Score Matchingâ€ (Oral), 2nd Interdisciplinary Biological Sciences Symposium, Ames, IA, 04/03/2024\nâ€œInvestigating Gender Disparities in Glioblastoma via Propensity Score Matchingâ€ (Lightning talk & Poster), Boston Pharmaceutical Symposium, Sanofi, Cambridge, MA, 10/13/2023.\nâ€œEquity in Focus: Investigating Gender Disparities in Glioblastoma via Propensity Score Matchingâ€ (Poster), Statistics in Pharmaceuticals Conference, University of Connecticut (Virtual), 08/17/2023.\nS. Eshun, T. Oraby, & M. Al-Mamun., â€œA Machine Learning Approach to Evaluate the Effect of SGLT2 on CKD in Diabetes Patientsâ€ (Poster), College of Sciences Annual Research Conference, Brownsville, TX, 04/28/2023.\nâ€œDiagnosing CKD in Type 2 Diabetes Patients: A Machine Learning Approachâ€ (Poster), 9th Annual TRACS Summit, Texas A&M, College Station, TX, 04/03/2023.\nâ€œDiagnosing CKD & Heart Failure in Type 2 Diabetes Patients: A Machine Learning Approachâ€ (Poster), School of Interdisciplinary Programs & Community Engagement (SIPCE) 2nd Annual Conference, Edinburg, TX, 03/30/2023."
  },
  {
    "objectID": "write_ups/ex1.html",
    "href": "write_ups/ex1.html",
    "title": "Propensity Score Matching in Python",
    "section": "",
    "text": "under construction !!!\n\nexang (Exercise Induced Angina): This variable is binary (1 = yes, 0 = no) and indicates whether the patient experienced angina (chest pain) induced by exercise. It is a good candidate for a treatment variable because angina can be considered as an â€œeventâ€ or â€œconditionâ€ whose impact on other health outcomes can be assessed.\nnum (Diagnosis of Heart Disease): This is the outcome variable and is already coded as 0 = no presence and 1-4 indicating different types of defects (often, any value &gt; 0 is considered as presence of disease). This variable can be used to determine the effect of exercise-induced angina on the presence of heart disease.\n\n\nimport pandas as pd\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n# Column names based on the dataset documentation\ncolumn_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n                'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n\n# Load the dataset\ndf = pd.read_csv(url, names=column_names)\n\nimport pandas as pd\n\n# Assuming df is already loaded with the heart disease data\n# Convert 'num' to binary: 0 indicates no heart disease, 1-4 are converted to 1 indicating presence of heart disease\ndf['num'] = (df['num'] &gt; 0).astype(int)\n\n# Verify the changes\nprint(df['num'].value_counts())\n\n\n# Check the first few rows of the dataframe\nprint(df.shape)\n\nimport numpy as np\n\n# Replace '?' with NaN\ndf.replace('?', np.nan, inplace=True)\n\n# Option to drop rows with NaN\ndf.dropna(inplace=True)\n\nprint(df.shape)\n\n\ndf\n\nnum\n0    164\n1    139\nName: count, dtype: int64\n(303, 14)\n(297, 14)\n\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\nnum\n\n\n\n\n0\n63.0\n1.0\n1.0\n145.0\n233.0\n1.0\n2.0\n150.0\n0.0\n2.3\n3.0\n0.0\n6.0\n0\n\n\n1\n67.0\n1.0\n4.0\n160.0\n286.0\n0.0\n2.0\n108.0\n1.0\n1.5\n2.0\n3.0\n3.0\n1\n\n\n2\n67.0\n1.0\n4.0\n120.0\n229.0\n0.0\n2.0\n129.0\n1.0\n2.6\n2.0\n2.0\n7.0\n1\n\n\n3\n37.0\n1.0\n3.0\n130.0\n250.0\n0.0\n0.0\n187.0\n0.0\n3.5\n3.0\n0.0\n3.0\n0\n\n\n4\n41.0\n0.0\n2.0\n130.0\n204.0\n0.0\n2.0\n172.0\n0.0\n1.4\n1.0\n0.0\n3.0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n297\n57.0\n0.0\n4.0\n140.0\n241.0\n0.0\n0.0\n123.0\n1.0\n0.2\n2.0\n0.0\n7.0\n1\n\n\n298\n45.0\n1.0\n1.0\n110.0\n264.0\n0.0\n0.0\n132.0\n0.0\n1.2\n2.0\n0.0\n7.0\n1\n\n\n299\n68.0\n1.0\n4.0\n144.0\n193.0\n1.0\n0.0\n141.0\n0.0\n3.4\n2.0\n2.0\n7.0\n1\n\n\n300\n57.0\n1.0\n4.0\n130.0\n131.0\n0.0\n0.0\n115.0\n1.0\n1.2\n2.0\n1.0\n7.0\n1\n\n\n301\n57.0\n0.0\n2.0\n130.0\n236.0\n0.0\n2.0\n174.0\n0.0\n0.0\n2.0\n1.0\n3.0\n1\n\n\n\n\n297 rows Ã— 14 columns"
  },
  {
    "objectID": "write_ups/ex2.html",
    "href": "write_ups/ex2.html",
    "title": "Cleaning Text Data in Python",
    "section": "",
    "text": "Outline\n\nIntroduction\nPython Implementation\n\n\n\nIntroduction\nIn sentiment analysis, the quality of input data can significantly influence the accuracy and effectiveness of the final outcomes. Analyzing texts, given their concise nature and informal language, presents unique challenges. This blog post details my approach to preprocessing texts to enhance sentiment analysis. Through a series of carefully designed steps, I aim to transform raw text data into a clean, analyzable format, setting the stage for accurate classification.\n\n\n\nFirst, I removed any URLs from the tweets since they do not contribute to sentiment analysis and only add noise and irrelevant information to the text.\nNext, I tokenized the tweets by splitting them into individual words. Tokenization allows for processing the text at a detailed level, treating each word as a separate entity, which is crucial for the cleaning and analysis that follows.\nAfter tokenization, I applied text cleaning steps to refine the data further. This included removing stopwordsâ€”common words like â€œthe,â€ â€œand,â€ â€œis,â€ which do not carry significant sentiment information. Eliminating these words helps reduce textual noise and focuses the analysis on more impactful words that convey sentiment.\nI also removed special characters, such as punctuation marks and symbols, from the text. These characters do not aid in sentiment analysis and can hinder the performance of further analyses.\nAdditionally, I performed lemmatization on the text, which reduces words to their base or root forms. This consolidates similar words and reduces dimensionality, improving the modelâ€™s ability to generalize across different variations of words. For instance, words like â€œrunning,â€ â€œran,â€ and â€œrunsâ€ are all reduced to the base form â€œrun.â€\nBy applying these text preprocessing techniques, I transformed the raw tweet data into a clean and standardized format. This preparation is crucial for accurate sentiment analysis as it helps to eliminate noise, reduce dimensionality, and concentrate on the most relevant text features. The preprocessed tweets are now ready for further analysis and sentiment classification using suitable models.\n\n\nPython codes implementation\nBefore creating the preprocessing function, itâ€™s essential to set up the environment with the required packages. This includes loading libraries and downloading necessary data for text manipulation.\n\n# Load Required packages\nimport pandas as pd\nimport numpy as np\nimport nltk, re\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport emoji\n\n# Download necessary NLTK resources\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/solomoneshun/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/solomoneshun/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/solomoneshun/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\nnltk: A toolkit for natural language processing that provides capabilities for tokenizing, stemming, and more.\nre: Helps in removing punctuation and non-alphabetical characters using regular expressions.\nstopwords: Common words in English that do not add much meaning to sentences and are often removed in the preprocessing step (eg. is, the, a, and, an, â€¦).\n\n\n\nDefining the Text Preprocessing Function\nThe main function, preprocess_text, cleans up the text data by removing unwanted characters, converting text to lowercase, removing stopwords, and lemmatizing the words.\n\ndef preprocess_text(data):\n  \n    processed_data = []\n    stop = set(stopwords.words('english'))  # Set of English stopwords\n\n    for k in range(len(data)):\n\n        # Remove URLs\n        text = re.sub(r'https?://\\S+|www\\.\\S+', '', data[k])\n\n        # Remove emojis using the emoji library\n        text = emoji.replace_emoji(text, replace='')\n\n        # Remove non-alphabet characters and convert to lowercase\n        text = re.sub('[^a-zA-Z]', ' ', text).lower().split()\n\n        # Lemmatize words and remove stopwords\n        lemmatizer = WordNetLemmatizer()\n        stem_list = [lemmatizer.lemmatize(i) for i in text if i not in stop]\n\n        # Rejoin words to form the final cleaned text\n        final_text = ' '.join(stem_list)\n        processed_data.append(final_text)\n\n    return processed_data\n\n\n\nSteps Explained:\n\nRegular Expression: re.sub('[^a-zA-Z]', ' ', data[k]) removes any character that is not a letter.\nLowercasing & Splitting: Converts the text to lowercase and splits into individual words.\nLemmatization: Reduces words to their base or root form.\nStopwords Removal: Filters out common words that are unlikely to contribute to sentiment analysis.\n\nTo demonstrate the functionality, the preprocessing function is applied to a sample dataset containing reviews. This will output the cleaned versions of the input reviews, showing how punctuation, common words, and case have been normalized.\n\n# Sample input text\nsample_text = [\"This place is amazing ðŸ˜ŠðŸ‘ .. @ 100% Check: https://example.com!! :)\"]\ncleaned_text = preprocess_text(sample_text)\nprint(cleaned_text)\n\n['place amazing check']\n\n\n\n\nCreating a Larger DataFrame and Cleaning Texts\nNext, we create a larger DataFrame with 20 sample reviews, ratings, and IDs, and apply the function to clean the review texts.\n\n# Generating sample data\ndata = {\n    'Review': [\n        \"Love this place, will come again!ðŸ‘\", \"Not coming again! very bad experience!ðŸ˜¡\",\n        \"Best experience ever, highly recommend!\", \"Worst place I have been to!\",\n        \"Just okay, nothing special.ðŸ™‚\", \"Fantastic service and food!\",\n        \"Would not recommend this place to anyone.\", \"Great location, mediocre food.\",\n        \"Terrible customer service.\", \"Amazing atmosphere, will return!\",\n        \"Overpriced for the quality.\", \"Perfect spot for a weekend getaway!\",\n        \"Dirty rooms and rude staff.\", \"Exceptional service and amenities!\",\n        \"Too noisy and crowded.\", \"Peaceful and relaxing environment.\",\n        \"Unfriendly staff and bad food.\", \"Loved the outdoor setting!\",\n        \"Cancellation policy is too strict.\", \"Excellent choice for families!\"\n    ],\n    'Rating': [5, 1, 5, 1, 3, 5, 1, 3, 1, 5, 2, 5, 1, 5, 2, 4, 1, 4, 2, 5]\n}\n\n# Creating the DataFrame\nreviews_df = pd.DataFrame(data)\n\n# Applying the text preprocessing function\nreviews_df['Cleaned_Review'] = preprocess_text(reviews_df['Review'].tolist())\nreviews_df\n\n\n\n\n\n\n\n\n\nReview\nRating\nCleaned_Review\n\n\n\n\n0\nLove this place, will come again!ðŸ‘\n5\nlove place come\n\n\n1\nNot coming again! very bad experience!ðŸ˜¡\n1\ncoming bad experience\n\n\n2\nBest experience ever, highly recommend!\n5\nbest experience ever highly recommend\n\n\n3\nWorst place I have been to!\n1\nworst place\n\n\n4\nJust okay, nothing special.ðŸ™‚\n3\nokay nothing special\n\n\n5\nFantastic service and food!\n5\nfantastic service food\n\n\n6\nWould not recommend this place to anyone.\n1\nwould recommend place anyone\n\n\n7\nGreat location, mediocre food.\n3\ngreat location mediocre food\n\n\n8\nTerrible customer service.\n1\nterrible customer service\n\n\n9\nAmazing atmosphere, will return!\n5\namazing atmosphere return\n\n\n10\nOverpriced for the quality.\n2\noverpriced quality\n\n\n11\nPerfect spot for a weekend getaway!\n5\nperfect spot weekend getaway\n\n\n12\nDirty rooms and rude staff.\n1\ndirty room rude staff\n\n\n13\nExceptional service and amenities!\n5\nexceptional service amenity\n\n\n14\nToo noisy and crowded.\n2\nnoisy crowded\n\n\n15\nPeaceful and relaxing environment.\n4\npeaceful relaxing environment\n\n\n16\nUnfriendly staff and bad food.\n1\nunfriendly staff bad food\n\n\n17\nLoved the outdoor setting!\n4\nloved outdoor setting\n\n\n18\nCancellation policy is too strict.\n2\ncancellation policy strict\n\n\n19\nExcellent choice for families!\n5\nexcellent choice family"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "NKstats",
    "section": "",
    "text": "Welcome to NKstats! This page provides information about my projects, goals, and background.\nThe full site is under construction â€¦"
  },
  {
    "objectID": "write_ups/ex1.html#footnotes",
    "href": "write_ups/ex1.html#footnotes",
    "title": "Propensity Score Matching in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Rosenbaum and Rubin (1983) for more details.â†©ï¸Ž\nSee more in Rosenbaum and Rubinâ€™s seminal work (Rosenbaum and Rubin 1983).â†©ï¸Ž"
  }
]