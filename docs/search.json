[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "NKstats",
    "section": "",
    "text": "Welcome to the NKstats Blog! Explore our latest posts below.\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nStill Working2!\n\n\n\nstatistics\n\n\nprogramming\n\n\n\n\n\n\n\nSolomon Eshun\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropensity Score Matching in Python\n\n\n\nprogramming\n\n\n\nA step-by-step approach to conducting propensity score matching in Python\n\n\n\nSolomon Eshun\n\n\nApr 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText Preprocessing for Sentiment Analysis\n\n\n\nprogramming\n\n\n\nA guide to pre-processing text data for sentiment analysis in Python.\n\n\n\nSolomon Eshun\n\n\nApr 11, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Readme.html",
    "href": "Readme.html",
    "title": "<<<<<<< HEAD",
    "section": "",
    "text": "&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n\n\n\n\n\n\n\n6e566f9dd1699045ba8fe7ac06b9bb05071f6302"
  },
  {
    "objectID": "write_ups/ex3.html",
    "href": "write_ups/ex3.html",
    "title": "Still Working2!",
    "section": "",
    "text": "under construction !"
  },
  {
    "objectID": "NKstats.html",
    "href": "NKstats.html",
    "title": "NKstats",
    "section": "",
    "text": "Hi! My name is Solomon Eshun (he/him/his). I am a PhD student in Applied Mathematics at Iowa State University.\nI am dedicated to developing statistical methods and tools that address public health challenges. My goal is to master methodologies that can identify the most effective treatments and interventions by analyzing data from observational studies, clinical trials and other sources. I am particularly interested in combining machine learning with causal methods to analyze the effects of policies and interventions on disease outcomes, addressing the challenges posed by heterogeneous patient characteristics and varying medical protocols within healthcare datasets. Additionally, I am interested in exploring the dynamics of infectious disease transmission by developing mathematical and statistical models to simulate disease spread and evaluate the effectiveness of control strategies.\n\n\n\n\n\n\n\n\nExpand All Collapse All\n\n\n\n\n\n\n    Research Interests\n    \n    Causal Inference, Precision Medicine, Machine Learning, Infectious Disease Modeling\n\n\n\n\n\n\nEducation\n\n\n    \n        PhD (Student) in Applied Mathematics\n        Iowa State University, Ames, 2023 - 2024\n     \n    \n        MS in Applied Statistics & Data Science\n        University of Texas Rio Grande Valley (UTRGV), Edinburg, 2023\n    \n    \n        BS in Mathematics\n        University of Mines and Technology (UMaT), Ghana, 2020\n    \n\n\n\n\n\n\n\nExperience\n\n\n    \n        Graduate Research Assistant, .......... (08/2024 - Present)\n     \n    \n        Graduate Fellow, Data Science for the Public Good, Iowa State (05/2024 - 07/2024)\n    \n    \n        Graduate Teaching Assistant, Department of Mathematics, Iowa State (08/2023 - 05/2023)\n    \n    \n    \n        Sustainability Research Fellow, Office for Sustainability, UTRGV (08/2022 - 05/2023)\n        \n    \n        Graduate Teaching Assistant, School of Mathematical & Statistical Sciences, UTRGV (08/2021 - 05/2023)\n        \n    \n        Computational Science Intern, Los Alamos National Lab, Los Alamos, NM (05/2022 - 08/2022)\n        \n    \n        Teaching Assistant, Department of Mathematical Sciences, UMaT (09/2020 - 08/2021)\n    \n        \n    \n        Data Production Intern, Ghana Statistical Service, Accra (05/2019 - 08/2019)\n    \n\n\n\n\n\n\nHonors & Awards\n\n\n    \n        The Cole, 1958 & Green Scholarships, 2024.\n    \n    \n    \n        Travel Award, Department of Mathematics, Iowa State University, 2023.\n    \n    \n    \n        Certificate of Excellence in Sustainable Development Graduate Research, UTRGV, 2023.\n    \n    \n    \n        UTRGV Applied Statistics & Data Science Outstanding M.S. Thesis Award, 2023.\n    \n    \n    \n        Poster Award, SIPCE 2nd Annual Conference for Interdisciplinary Research, UTRGV, 2023.\n    \n    \n    \n        Sustainability Research Fellowship, Office for Sustainability, UTRGV, 2022.\n    \n    \n    \n        Burkhart Endowed Scholarship, School of Mathematical & Statistical Sciences, UTRGV, 2022.\n    \n    \n    \n        Joseph Wiener Math Scholarship, School of Mathematical & Statistical Sciences, UTRGV, 2022.\n    \n    \n    \n        Vice Chancellor's Overall Best Graduating Student, University of Mines & Technology, 2020.\n    \n    \n    \n        Best Graduating Student, Faculty of Engineering, University of Mines & Technology, 2020.\n    \n    \n    \n        Best Student, Department of Mathematical Sciences, University of Mines & Technology, 2020."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "NKstats",
    "section": "",
    "text": "Publications\n\n\n\n\nS. Eshun. Equity in Focus: Investigating Gender Disparities in Glioblastoma via Propensity Score Matching. JMIRx Med (In Review: since August 15, 2023).\nS. Eshun, R. Essieku, J. Ladzekpo (2023). Stability Analyses on the Effect of Vaccination and Contact Tracing in Monkeypox Virus Transmission. Journal of Mathematical & Computational Science, 13. Paper Link\nD. Florez, E. Romero-Severson, K.M. Martinez, C. Franco, S. Eshun, J.A. Spencer, C. Manore, M. Mancuso, J. Keithley (2022). Assessing the Risk of Dengue Outbreaks across Continental Biomes in Brazil. In AGU Fall Meeting Abstracts. 2022, GH25D-0625. Abstract Link"
  },
  {
    "objectID": "conference.html",
    "href": "conference.html",
    "title": "NKstats",
    "section": "",
    "text": "Conference Presentations\n\n\n\n\n“Estimating Gender Differences in Glioblastoma Outcomes using Propensity Score Matching” (Oral), 2nd Interdisciplinary Biological Sciences Symposium, Ames, IA, 04/03/2024\n“Investigating Gender Disparities in Glioblastoma via Propensity Score Matching” (Lightning talk & Poster), Boston Pharmaceutical Symposium, Sanofi, Cambridge, MA, 10/13/2023.\n“Equity in Focus: Investigating Gender Disparities in Glioblastoma via Propensity Score Matching” (Poster), Statistics in Pharmaceuticals Conference, University of Connecticut (Virtual), 08/17/2023.\nS. Eshun, T. Oraby, & M. Al-Mamun., “A Machine Learning Approach to Evaluate the Effect of SGLT2 on CKD in Diabetes Patients” (Poster), College of Sciences Annual Research Conference, Brownsville, TX, 04/28/2023.\n“Diagnosing CKD in Type 2 Diabetes Patients: A Machine Learning Approach” (Poster), 9th Annual TRACS Summit, Texas A&M, College Station, TX, 04/03/2023.\n“Diagnosing CKD & Heart Failure in Type 2 Diabetes Patients: A Machine Learning Approach” (Poster), School of Interdisciplinary Programs & Community Engagement (SIPCE) 2nd Annual Conference, Edinburg, TX, 03/30/2023."
  },
  {
    "objectID": "write_ups/ex1.html",
    "href": "write_ups/ex1.html",
    "title": "Propensity Score Matching in Python",
    "section": "",
    "text": "under construction !!!\n\nexang (Exercise Induced Angina): This variable is binary (1 = yes, 0 = no) and indicates whether the patient experienced angina (chest pain) induced by exercise. It is a good candidate for a treatment variable because angina can be considered as an “event” or “condition” whose impact on other health outcomes can be assessed.\nnum (Diagnosis of Heart Disease): This is the outcome variable and is already coded as 0 = no presence and 1-4 indicating different types of defects (often, any value &gt; 0 is considered as presence of disease). This variable can be used to determine the effect of exercise-induced angina on the presence of heart disease.\n\n\nimport pandas as pd\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n# Column names based on the dataset documentation\ncolumn_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n                'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n\n# Load the dataset\ndf = pd.read_csv(url, names=column_names)\n\nimport pandas as pd\n\n# Assuming df is already loaded with the heart disease data\n# Convert 'num' to binary: 0 indicates no heart disease, 1-4 are converted to 1 indicating presence of heart disease\ndf['num'] = (df['num'] &gt; 0).astype(int)\n\n# Verify the changes\nprint(df['num'].value_counts())\n\n\n# Check the first few rows of the dataframe\nprint(df.shape)\n\nimport numpy as np\n\n# Replace '?' with NaN\ndf.replace('?', np.nan, inplace=True)\n\n# Option to drop rows with NaN\ndf.dropna(inplace=True)\n\nprint(df.shape)\n\n\ndf\n\nnum\n0    164\n1    139\nName: count, dtype: int64\n(303, 14)\n(297, 14)\n\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\nnum\n\n\n\n\n0\n63.0\n1.0\n1.0\n145.0\n233.0\n1.0\n2.0\n150.0\n0.0\n2.3\n3.0\n0.0\n6.0\n0\n\n\n1\n67.0\n1.0\n4.0\n160.0\n286.0\n0.0\n2.0\n108.0\n1.0\n1.5\n2.0\n3.0\n3.0\n1\n\n\n2\n67.0\n1.0\n4.0\n120.0\n229.0\n0.0\n2.0\n129.0\n1.0\n2.6\n2.0\n2.0\n7.0\n1\n\n\n3\n37.0\n1.0\n3.0\n130.0\n250.0\n0.0\n0.0\n187.0\n0.0\n3.5\n3.0\n0.0\n3.0\n0\n\n\n4\n41.0\n0.0\n2.0\n130.0\n204.0\n0.0\n2.0\n172.0\n0.0\n1.4\n1.0\n0.0\n3.0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n297\n57.0\n0.0\n4.0\n140.0\n241.0\n0.0\n0.0\n123.0\n1.0\n0.2\n2.0\n0.0\n7.0\n1\n\n\n298\n45.0\n1.0\n1.0\n110.0\n264.0\n0.0\n0.0\n132.0\n0.0\n1.2\n2.0\n0.0\n7.0\n1\n\n\n299\n68.0\n1.0\n4.0\n144.0\n193.0\n1.0\n0.0\n141.0\n0.0\n3.4\n2.0\n2.0\n7.0\n1\n\n\n300\n57.0\n1.0\n4.0\n130.0\n131.0\n0.0\n0.0\n115.0\n1.0\n1.2\n2.0\n1.0\n7.0\n1\n\n\n301\n57.0\n0.0\n2.0\n130.0\n236.0\n0.0\n2.0\n174.0\n0.0\n0.0\n2.0\n1.0\n3.0\n1\n\n\n\n\n297 rows × 14 columns\n\n\n\n\n\nimport researchpy as rp\n\n# rp.summary_cat(df['num'])\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncovariate = 'thalach'\n\ndata = df\n\npepe_calories = data[data['exang'] == 1][covariate]\nmodern_calories = data[data['exang'] == 0][covariate]\nplt.figure(figsize=(8,4))\nax = sns.distplot(pepe_calories, fit_kws={\"color\":\"'orange'\"}, kde=True,#palette=[\"#3498db\",'orange']\n        hist=None, label=\"exang = 0\", kde_kws={\"linewidth\": 2.5});\nax = sns.distplot(modern_calories, fit_kws={\"color\":\"#3498db\"}, kde=True,\n        hist=None, label=\"exang = 1\", kde_kws={\"linewidth\": 2.5});\n\nplt.xlabel('Propensity Score')\nplt.legend(fontsize=10)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.xlabel('Estimated Ps', fontsize=10, labelpad=10)\nplt.ylabel(' ', fontsize=10, labelpad=10)\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nplt.yticks([])\nplt.title('Before Matching', fontsize=10)\nplt.show()\n\n\n\n\n\n\n\n\n\ncontinuous_vars = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n\n# Converting all other variables to 'category' data type except the ones in continuous_vars\nfor column in df.columns:\n    if column not in continuous_vars:\n        df[column] = df[column].astype('category')\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming df is your DataFrame after loading the data with the correct column names\n\n# Setting the aesthetic style of the plots\n# sns.set(style=\"whitegrid\")\n\n# Determine the number of subplots\nn_vars = df.columns.size\nn_cols = 2\nn_rows = (n_vars + n_cols - 1) // n_cols  # This ensures enough rows to accommodate all plots\n\n# Create a figure to hold the subplots\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\naxes = axes.flatten()  # Flatten the axes array for easy iteration\n\n# Iterate through the list of columns and create plots\nfor i, column in enumerate(df.columns):\n    ax = axes[i]\n    if df[column].dtype in ['int64', 'float64']:  # Adjust based on your specific data types\n        sns.histplot(data=df, x=column, hue='exang', element='step', stat='density', common_norm=False, ax=ax)\n        ax.set_title(f'Distribution of {column} by Exang')\n        ax.set_xlabel(column)\n        ax.set_ylabel('Density')\n        ax.legend(title='Exang', labels=['No Angina (0)', 'With Angina (1)'])\n    else:\n        sns.countplot(x=column, hue='exang', data=df, ax=ax)\n        ax.set_title(f'Count of {column} by Exang')\n        ax.set_xlabel(column)\n        ax.set_ylabel('Count')\n        ax.legend(title='Exang', labels=['No Angina (0)', 'With Angina (1)'])\n\n# Adjust layout to prevent overlap and ensure labels are readable\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIntroduction\nIn statistical analysis, the challenge of comparing treatments in observational studies is significant due to the presence of confounding variables. Unlike randomized controlled trials (RCTs), observational studies do not randomly assign treatments, which can lead to biased estimates of treatment effects. This is where the idea of propensity score matching (PSM) becomes an invaluable tool.\nPropensity score matching is a statistical technique that attempts to estimate the effect of a treatment or intervention by accounting for the covariates that predict receiving the treatment (or intervention). It was developed by Rosenbaum and Rubin (1983) to help reduce bias by matching units receiving the treatment with similar units that do not receive the treatment based on a calculated score known as the propensity score.\nThis score is the probability of treatment assignment conditional on observed baseline characteristics. This matching process helps to form a sample in which the distribution of observed baseline covariates is similar between treated and untreated groups, thereby mimicking a randomized experiment.\nIn this blog, I will dive into how to implement propensity score matching in R. I will cover the essential packages, step-by-step coding examples, and practical tips to ensure efficient analysis. Whether you’re a researcher, statistician, or data enthusiast, understanding how to effectively conduct PSM in R can significantly enhance your analytical capabilities in dealing with observational data.\nLet’s get started by exploring the basics of propensity score calculation and then move into how to perform matching using R’s comprehensive statistical packages. Ho et al. (2011)\n(Ho et al. 2011)\n\n\nThesis\nCausal inference is a vital aspect of medical research that helps to establish causal relationships between different variables. It is the process of making claims about cause-and-effect relationships in a given system or phenomenon. It is an essential component of scientific research, particularly in fields such as medicine, economics, and social sciences, where experiments that manipulate variables are often not feasible or ethical. In these cases, researchers often rely on comparative experimental studies.\nObservational studies become important in fields such as medicine, economics, and social sciences where it may not be feasible or ethical to conduct experiments that manipulate variables. However, making causal inferences in observational studies can be challenging due to limitations such as random selection of subjects but not random allocation of treatments to subjects. This makes it difficult to determine whether the difference in outcome between treated and untreated subjects is due to the treatment or differences in other characteristics of the subjects.\nAnother limitation is self-selection, where individuals opt for a particular treatment for specific reasons, making it difficult to compare them directly with those who did not receive the treatment.\nThere is an increasing interest in estimating the causal effects of treatment using observational or nonrandomized data. In observational studies, the baseline characteristics of treated or exposed subjects often differ systematically from those of untreated or unexposed subjects. Essential to the production of high-quality evidence to inform decision-making is the ability to minimize the effect of confounding. An increasingly frequent approach to minimizing bias when estimating causal treatment effects is based on the propensity score (the likelihood of receiving treatment based on some outlines subject characteristics) usually obtained using a logit model Rosenbaum and Rubin (1983).\nPropensity score matching have gained much relevance across disciplines to estimate causal effects using observational data. This technique attempt to replicate the ideal of randomized experiments as closely as possible. It a quasi-experimental method that aims to search for counterfactual unit that is comparable with the treated unit among many untreated units.\n\n\n\n\n\nReferences\n\nHo, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2011. “MatchIt: Nonparametric Preprocessing for Parametric Causal Inference.” Journal of Statistical Software 42 (8): 1–28. https://doi.org/10.18637/jss.v042.i08.\n\n\nRosenbaum, Paul R, and Donald B Rubin. 1983. “The Central Role of the Propensity Score in Observational Studies for Causal Effects.” Biometrika 70 (1): 41–55. https://doi.org/10.2307/2335942."
  },
  {
    "objectID": "write_ups/ex2.html",
    "href": "write_ups/ex2.html",
    "title": "Cleaning Text Data in Python",
    "section": "",
    "text": "Outline\n\nIntroduction & Method\nPython Implementation\n\n\n\nIntroduction & Method\nIn sentiment analysis, the quality of input data can significantly influence the accuracy and effectiveness of the final outcomes. Analyzing texts, given their concise nature and informal language, presents unique challenges. This post details my approach to preprocessing texts to enhance sentiment analysis. Through a series of carefully designed steps, I aim to transform raw text data into a clean, analyzable format, setting the stage for accurate classification.\n\n\n\nHere’s a detailed breakdown of the preprocessing techniques I employed to refine the raw tweet data:\n\nURL Removal: I began by stripping URLs from the texts. URLs usually don’t contribute to sentiment and can add irrelevant noise to the text, so removing them helps in focusing on meaningful content.\nTokenization: Next, I tokenized the text by splitting the texts into individual words. This is crucial as it allows us to process the text at the word level, which is essential for the subsequent cleaning and analysis steps.\nRemoving Stopwords: I then removed stopwords - common words such as “the,” “and,” “is,” etc., which are abundant in English but do not carry significant sentiment. Eliminating these words helps reduce textual noise and focuses the analysis on more impactful words that convey sentiment.\nEliminating Special Characters: I also scrubbed the text of special characters like punctuation marks, symbols and emojis. These characters do not typically contribute to sentiment and can impede the performance of sentiment analysis models.\nLemmatization: Finally, I applied lemmatization to consolidate similar words to their base or root forms. This process reduces the number of unique words the model must handle, enhancing its ability to generalize across different variations of the same word. For example, “running,” “ran,” and “runs” are all reduced to “run.”\n\nThrough these text preprocessing steps, I transformed the raw text data into a clean and standardized format. This preparation is vital for conducting accurate sentiment analysis as it helps eliminate noise, reduce the complexity of the data, and focus on the most impactful elements of the text. The preprocessed texts are now ready for further analysis and can be effectively used in sentiment classification models to understand public sentiment.\n\n\nPython codes implementation\nBefore creating the preprocessing function, it’s essential to set up the environment with the required packages. This includes loading libraries and downloading necessary data for text manipulation.\n\n# Install the emoji package before loading\n!pip install emoji\n\n\n# Load Required packages\nimport pandas as pd\nimport numpy as np\nimport nltk, re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport emoji\n\n# Download necessary NLTK resources\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n\n\n\nDefining the Text Preprocessing Function\nThe main function, preprocess_text, cleans up the text data by removing unwanted characters, converting text to lowercase, removing stopwords, and lemmatizing the words. It incorporates the step-by-step approach discussed in Introduction & Method\n\ndef preprocess_text(data):\n  \n    processed_data = []\n    lemmatizer = WordNetLemmatizer()\n    stop = set(stopwords.words('english'))  # Set of English stopwords\n\n    for sentence in data:\n\n        # Remove URLs\n        text = re.sub(r'https?://\\S+|www\\.\\S+', '', sentence)\n\n        # Remove emojis using the emoji library\n        text = emoji.replace_emoji(text, replace='')\n\n        # Remove non-alphabet characters and convert to lowercase\n        text = re.sub('[^a-zA-Z]', ' ', text).lower().split()\n\n        # Lemmatize words and remove stopwords\n        stem_list = [lemmatizer.lemmatize(i) for i in text if i not in stop]\n\n        # Rejoin words to form the final cleaned text\n        final_text = ' '.join(stem_list)\n        processed_data.append(final_text)\n\n    return processed_data\n\n\n\nSteps Explained:\n\nRegular Expression: re.sub('[^a-zA-Z]', ' ', data[k]) removes any character that is not a letter.\nLowercasing & Splitting: Converts the text to lowercase and splits into individual words.\nLemmatization: Reduces words to their base or root form.\nStopwords Removal: Filters out common words that are unlikely to contribute to sentiment analysis.\n\nTo demonstrate the functionality, the preprocessing function is applied to a sample dataset containing reviews. This will output the cleaned versions of the input reviews, showing how punctuation, common words, and case have been normalized.\n\n# Sample input text\nsample_text = [\"This place is amazing 😊👍🌍.. @100% Check: https://example.com!! :)\"]\ncleaned_text = preprocess_text(sample_text)\nprint(cleaned_text)\n\n['place amazing check']\n\n\n\n\nCreating a Larger DataFrame and Cleaning Texts\nNext, we create a larger DataFrame with 20 sample reviews, ratings, and IDs, and apply the function to clean the review texts.\n\n# Generating sample data\ndata = {\n    'Review': [\n        \"Love this place, will come again!👍\", \"Not again! very bad experience!😡\",\n        \"Best experience ever, highly recommend!\", \"Worst place I have been to!\",\n        \"Just okay, nothing special.🙂\", \"Fantastic service and food!\",\n        \"Would not recommend this place to anyone.\", \"Great location, mediocre food.\",\n        \"Terrible customer service.\", \"Amazing atmosphere, will return!\",\n        \"Overpriced for the quality.\", \"Perfect spot for a weekend getaway!\",\n        \"Dirty rooms and rude staff.\", \"Exceptional service and amenities!\",\n        \"Too noisy and crowded.\", \"Peaceful and relaxing environment.\",\n        \"Unfriendly staff and bad food.\", \"Loved the outdoor setting!\",\n        \"Cancellation policy is too strict.\", \"Excellent choice for families!\"\n    ],\n    'Rating': [5, 1, 5, 1, 3, 5, 1, 3, 1, 5, 2, 5, 1, 5, 2, 4, 1, 4, 2, 5]\n}\n\n# Creating the DataFrame\nreviews_df = pd.DataFrame(data)\n\n# Applying the text preprocessing function\nreviews_df['Cleaned_Review'] = preprocess_text(reviews_df['Review'].tolist())\nreviews_df\n\n\n\n\n\n\n\n\n\nReview\nRating\nCleaned_Review\n\n\n\n\n0\nLove this place, will come again!👍\n5\nlove place come\n\n\n1\nNot again! very bad experience!😡\n1\nbad experience\n\n\n2\nBest experience ever, highly recommend!\n5\nbest experience ever highly recommend\n\n\n3\nWorst place I have been to!\n1\nworst place\n\n\n4\nJust okay, nothing special.🙂\n3\nokay nothing special\n\n\n5\nFantastic service and food!\n5\nfantastic service food\n\n\n6\nWould not recommend this place to anyone.\n1\nwould recommend place anyone\n\n\n7\nGreat location, mediocre food.\n3\ngreat location mediocre food\n\n\n8\nTerrible customer service.\n1\nterrible customer service\n\n\n9\nAmazing atmosphere, will return!\n5\namazing atmosphere return\n\n\n10\nOverpriced for the quality.\n2\noverpriced quality\n\n\n11\nPerfect spot for a weekend getaway!\n5\nperfect spot weekend getaway\n\n\n12\nDirty rooms and rude staff.\n1\ndirty room rude staff\n\n\n13\nExceptional service and amenities!\n5\nexceptional service amenity\n\n\n14\nToo noisy and crowded.\n2\nnoisy crowded\n\n\n15\nPeaceful and relaxing environment.\n4\npeaceful relaxing environment\n\n\n16\nUnfriendly staff and bad food.\n1\nunfriendly staff bad food\n\n\n17\nLoved the outdoor setting!\n4\nloved outdoor setting\n\n\n18\nCancellation policy is too strict.\n2\ncancellation policy strict\n\n\n19\nExcellent choice for families!\n5\nexcellent choice family"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "NKstats",
    "section": "",
    "text": "Welcome to NKstats! This page provides information about my projects, goals, and background.\nThe full site is under construction …"
  },
  {
    "objectID": "write_ups/ex1.html#footnotes",
    "href": "write_ups/ex1.html#footnotes",
    "title": "Propensity Score Matching in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Rosenbaum and Rubin (1983) for more details.↩︎\nSee more in Rosenbaum and Rubin’s seminal work (Rosenbaum and Rubin 1983).↩︎"
  },
  {
    "objectID": "write_ups/ex2.html#outline",
    "href": "write_ups/ex2.html#outline",
    "title": "Text Preprocessing for Sentiment Analysis",
    "section": "Outline",
    "text": "Outline\n\nIntroduction & Method\nPython Implementation"
  },
  {
    "objectID": "write_ups/ex2.html#Introduction-1",
    "href": "write_ups/ex2.html#Introduction-1",
    "title": "Text Preprocessing for Sentiment Analysis",
    "section": "Introduction & Method",
    "text": "Introduction & Method\nSentiment analysis is a critical component of natural language processing (NLP) that helps decipher the emotional tone behind words. This analysis is particularly valuable not only in social media monitoring and market research but also in public health initiatives. Understanding public sentiment can significantly enhance health communication strategies, patient engagement, and policy formulation. Moreover, in public health crises, sentiment analysis can help track community reactions to health advisories or public sentiment toward vaccination drives, thus informing more targeted health communication and intervention strategies. However, the effectiveness of sentiment analysis in these sensitive areas is heavily reliant on the quality of preprocessing applied to text data. This post details my approach to preprocessing texts to enhance sentiment analysis. Through a series of carefully designed steps, I aim to transform raw text data into a clean, analyzable format, setting the stage for accurate classification.\n\n\n\nHere’s a detailed breakdown of the preprocessing techniques I employed to refine the raw tweet data:\n\nURL Removal: I began by stripping URLs from the texts. URLs usually don’t contribute to sentiment and can add irrelevant noise to the text, so removing them helps in focusing on meaningful content.\nTokenization: Next, I tokenized the text by splitting the texts into individual words. This is crucial as it allows us to process the text at the word level, which is essential for the subsequent cleaning and analysis steps.\nRemoving Stopwords: I then removed stopwords - common words such as “the,” “and,” “is,” etc., which are abundant in English but do not carry significant sentiment. Eliminating these words helps reduce textual noise and focuses the analysis on more impactful words that convey sentiment.\nEliminating Special Characters: I also scrubbed the text of special characters like punctuation marks, symbols and emojis. These characters do not typically contribute to sentiment and can impede the performance of sentiment analysis models.\nLemmatization: Finally, I applied lemmatization to consolidate similar words to their base or dictionary forms. Unlike stemming, which simply chops off the ends of words to reach a common base, lemmatization considers the context and morphological analysis of words to accurately transform them to their lemma. This process reduces the number of unique words the model must handle, enhancing its ability to generalize across different variations of the same word.\n\nThrough these text preprocessing steps, I transformed the raw text data into a clean and standardized format. This preparation is vital for conducting accurate sentiment analysis as it helps eliminate noise, reduce the complexity of the data, and focus on the most impactful elements of the text. The preprocessed texts are now ready for further analysis and can be effectively used in sentiment classification models to understand public sentiment."
  },
  {
    "objectID": "write_ups/ex2.html#Python-codes-implementation",
    "href": "write_ups/ex2.html#Python-codes-implementation",
    "title": "Text Preprocessing for Sentiment Analysis",
    "section": "Python Implementation",
    "text": "Python Implementation\nBefore creating the preprocessing function, it’s essential to set up the environment with the required packages. This includes loading libraries and downloading necessary data for text manipulation.\n\n# Install the emoji package before loading\n!pip install emoji\n\n\n# Load Required packages\nimport pandas as pd\nimport numpy as np\nimport nltk, re, emoji, spacy\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n# Download necessary NLTK resources\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n\n\n\n\nThe preprocess_text function below incorporates all the preprocessing steps discussed in the Introduction & Method section. It is structured to be reusable for any text dataset that requires cleaning for sentiment analysis.\n\n# Load the spaCy model\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef preprocess_text(data):\n    processed_data = []\n    # Set of English stopwords\n    stop = set(stopwords.words('english'))\n    for sentence in data:\n        # Remove URLs\n        text = re.sub(r'https?://\\S+|www\\.\\S+', '', sentence)\n\n        # Remove emojis using the emoji library\n        text = emoji.replace_emoji(text, replace='')\n\n        # Create spaCy doc object\n        doc = nlp(text)\n\n        lem_list = []\n        for token in doc:\n            if token.is_alpha and token.text.lower() not in stop:  # Check if token is alphabetic and not a stopword\n                if token.pos_ != \"PRON\":  # Exclude all pronouns\n                    lem_list.append(token.lemma_.lower())\n\n        # Rejoin words to form the final cleaned text\n        final_text = ' '.join(lem_list)\n        processed_data.append(final_text)\n\n    return processed_data\n\n\n\n\nTo demonstrate the functionality, the preprocessing function is applied to a sample dataset containing reviews. This will output the cleaned versions of the input reviews, showing how punctuation, common words, and case have been normalized.\n\n# Sample input text\nsample_text = [\"The pandemic has affected the world.. @100% https://example.com!! :)\"]\ncleaned_text = preprocess_text(sample_text)\nprint(cleaned_text)\n\n['pandemic affect world']\n\n\n\n\n\nNext, we create a larger DataFrame with 20 sample reviews, ratings, and IDs, and apply the function to clean the review texts.\n\n# Creating a sample DataFrame with texts\n\ndata = {\n    'Review': [\n        \"Lockdown was necessary for our safety, I fully support it! 😊\",\n        \"I understand the need but it was too long. 😕\",\n        \"Lockdown completely disrupted my life, it was terrible. 😡\",\n        \"It was well-handled and necessary for public health. 🏥\",\n        \"Lockdown helped us control the virus spread. 😷\",\n        \"It was too restrictive and unnecessary for so long. 😒\",\n        \"Not sure if lockdown was the best solution, but we had no choice. 🤷\",\n        \"It saved lives, but at a great personal cost to many. 😔\",\n        \"The government did what was necessary during the lockdown. 👍\",\n        \"Lockdown was too harsh and not managed well. 😠\",\n        \"It was a good measure, but I hated being stuck at home. 🏠😖\",\n        \"Lockdown was essential but mental health suffered a lot. 🧠💔\",\n        \"The benefits outweigh the negatives of the lockdown. ➕➖\",\n        \"Poor execution made it harder than it should be. 😞\",\n        \"Lockdown was over before it was truly safe to do so. 😨\",\n        \"We needed the lockdown to protect vulnerable populations. 🛡️\",\n        \"The economic impact of lockdown was devastating. 💸\",\n        \"Lockdown showed we can take collective action when needed. 🤝\",\n        \"It was necessary, but the government support was insufficient. 💔\",\n        \"Lockdown should have been stricter to be more effective. ⚠️\"\n    ],\n    'Sentiment_Rating': [5, 3, 1, 4, 5, 2, 3, 3, 4, 1, 3, 2, 4, 2, 3, 5, 1, 4, 2, 4]\n}\n\n# Creating the DataFrame\nreviews_df = pd.DataFrame(data)\n\n# Applying the text preprocessing function\nreviews_df['Cleaned_Review'] = preprocess_text(reviews_df['Review'].tolist())\npd.set_option('display.max_colwidth', None)\nreviews_df\n\n\n\n\n\n\n\n\n\nReview\nSentiment_Rating\nCleaned_Review\n\n\n\n\n0\nLockdown was necessary for our safety, I fully support it! 😊\n5\nlockdown necessary safety fully support\n\n\n1\nI understand the need but it was too long. 😕\n3\nunderstand need long\n\n\n2\nLockdown completely disrupted my life, it was terrible. 😡\n1\nlockdown completely disrupt life terrible\n\n\n3\nIt was well-handled and necessary for public health. 🏥\n4\nwell handle necessary public health\n\n\n4\nLockdown helped us control the virus spread. 😷\n5\nlockdown help control virus spread\n\n\n5\nIt was too restrictive and unnecessary for so long. 😒\n2\nrestrictive unnecessary long\n\n\n6\nNot sure if lockdown was the best solution, but we had no choice. 🤷\n3\nsure lockdown good solution choice\n\n\n7\nIt saved lives, but at a great personal cost to many. 😔\n3\nsave life great personal cost many\n\n\n8\nThe government did what was necessary during the lockdown. 👍\n4\ngovernment necessary lockdown\n\n\n9\nLockdown was too harsh and not managed well. 😠\n1\nlockdown harsh manage well\n\n\n10\nIt was a good measure, but I hated being stuck at home. 🏠😖\n3\ngood measure hate stick home\n\n\n11\nLockdown was essential but mental health suffered a lot. 🧠💔\n2\nlockdown essential mental health suffer lot\n\n\n12\nThe benefits outweigh the negatives of the lockdown. ➕➖\n4\nbenefit outweigh negative lockdown\n\n\n13\nPoor execution made it harder than it should be. 😞\n2\npoor execution make hard\n\n\n14\nLockdown was over before it was truly safe to do so. 😨\n3\nlockdown truly safe\n\n\n15\nWe needed the lockdown to protect vulnerable populations. 🛡️\n5\nneed lockdown protect vulnerable population\n\n\n16\nThe economic impact of lockdown was devastating. 💸\n1\neconomic impact lockdown devastating\n\n\n17\nLockdown showed we can take collective action when needed. 🤝\n4\nlockdown show take collective action need\n\n\n18\nIt was necessary, but the government support was insufficient. 💔\n2\nnecessary government support insufficient\n\n\n19\nLockdown should have been stricter to be more effective. ⚠️\n4\nlockdown strict effective"
  }
]