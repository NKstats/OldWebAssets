---
title: "Text Preprocessing for Sentiment Analysis"
format: html
jupyter: python3
author: "Solomon Eshun"
date: 2022-04-11
description: "A guide to pre-processing text data for sentiment analysis in Python."
image: /imgs/senti.png
categories: ["programming"]
tags: [" "]
output: 
  html_document:
    toc: true
    toc_float: true
    smart: false
    print_df: paged
---


<style>
.circle-img {
  border-radius: 50%;
  width: 300px;
  height: 280px;
  object-fit: cover;
}
body {
    font-size: 16px;
}
.justify-text {
  text-align: justify;
  text-justify: inter-word;
}
details {
  position: relative;
}
details > summary {
  color: #333333;
  font-size: 24px;
  list-style: none;
  cursor: pointer;
  outline: none;
}
details > summary::-webkit-details-marker {
  display: none; 
}
details > summary:before {
  content: "+";
  display: inline-block;
  width: 25px; 
  height: 25px;
  line-height: 25px;
  text-align: center;
  background: #87CEEB; 
  border: 1px solid #ccc;
  margin-right: 10px;
  font-size: 16px;
}
details[open] > summary:before {
  content: "‚àí";
}
</style>



<div style="height: 20px;"></div>

## Outline

1. [Introduction & Method](#Introduction-1)
2. [Python Implementation](#Python-codes-implementation)


## Introduction & Method {#Introduction-1}

Sentiment analysis is a critical component of natural language processing (NLP) that helps decipher the emotional tone behind words. This analysis is particularly valuable not only in social media monitoring and market research but also in public health initiatives. Understanding public sentiment can significantly enhance health communication strategies, patient engagement, and policy formulation. Moreover, in public health crises, sentiment analysis can help track community reactions to health advisories or public sentiment toward vaccination drives, thus informing more targeted health communication and intervention strategies. However, the effectiveness of sentiment analysis in these sensitive areas is heavily reliant on the quality of preprocessing applied to text data. This post details my approach to preprocessing texts to enhance sentiment analysis. Through a series of carefully designed steps, I aim to transform raw text data into a clean, analyzable format, setting the stage for accurate classification.

<div style="text-align: center;">
  <img src="/imgs/Fig1.png" width="85%">
</div>

Here‚Äôs a detailed breakdown of the preprocessing techniques I employed to refine the raw tweet data:

1. <u>URL Removal</u>: I began by stripping URLs from the texts. URLs usually don't contribute to sentiment and can add irrelevant noise to the text, so removing them helps in focusing on meaningful content.


2. <u>Tokenization</u>: Next, I tokenized the text by splitting the texts into individual words. This is crucial as it allows us to process the text at the word level, which is essential for the subsequent cleaning and analysis steps.

3. <u>Removing Stopwords</u>: I then removed stopwords - common words such as "the," "and," "is," etc., which are abundant in English but do not carry significant sentiment. Eliminating these words helps reduce textual noise and focuses the analysis on more impactful words that convey sentiment.

4. <u>Eliminating Special Characters</u>: I also scrubbed the text of special characters like punctuation marks, symbols and emojis. These characters do not typically contribute to sentiment and can impede the performance of sentiment analysis models.

5. <u>Lemmatization</u>: Finally, I applied lemmatization to consolidate similar words to their base or dictionary forms. Unlike stemming, which simply chops off the ends of words to reach a common base, lemmatization considers the context and morphological analysis of words to accurately transform them to their lemma. This process reduces the number of unique words the model must handle, enhancing its ability to generalize across different variations of the same word. 

Through these text preprocessing steps, I transformed the raw text data into a clean and standardized format. This preparation is vital for conducting accurate sentiment analysis as it helps eliminate noise, reduce the complexity of the data, and focus on the most impactful elements of the text. The preprocessed texts are now ready for further analysis and can be effectively used in sentiment classification models to understand public sentiment.



## Python Implementation {#Python-codes-implementation}

Before creating the preprocessing function, it's essential to set up the environment with the required packages. This includes loading libraries and downloading necessary data for text manipulation.

```{python}
#| eval: false

# Install the emoji package before loading
!pip install emoji

```

```{python}

#| output: false

# Load Required packages
import pandas as pd
import numpy as np
import nltk, re, emoji, spacy
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

```

<div style="height: 20px;"></div>

The `preprocess_text` function below incorporates all the preprocessing steps discussed in the [Introduction & Method](#Introduction-1) section. It is structured to be reusable for any text dataset that requires cleaning for sentiment analysis.


```{python}

# Load the spaCy model
nlp = spacy.load("en_core_web_sm")

def preprocess_text(data):
    processed_data = []
    # Set of English stopwords
    stop = set(stopwords.words('english'))
    for sentence in data:
        # Remove URLs
        text = re.sub(r'https?://\S+|www\.\S+', '', sentence)

        # Remove emojis using the emoji library
        text = emoji.replace_emoji(text, replace='')

        # Create spaCy doc object
        doc = nlp(text)

        lem_list = []
        for token in doc:
            if token.is_alpha and token.text.lower() not in stop:  # Check if token is alphabetic and not a stopword
                if token.pos_ != "PRON":  # Exclude all pronouns
                    lem_list.append(token.lemma_.lower())

        # Rejoin words to form the final cleaned text
        final_text = ' '.join(lem_list)
        processed_data.append(final_text)

    return processed_data



  
```

<div style="height: 20px;"></div>

To demonstrate the functionality, the preprocessing function is applied to a sample dataset containing reviews. This will output the cleaned versions of the input reviews, showing how punctuation, common words, and case have been normalized.

```{python}

# Sample input text
sample_text = ["The pandemic has affected the world.. @100% https://example.com!! :)"]
cleaned_text = preprocess_text(sample_text)
cleaned_text

```

<div style="height: 20px;"></div>

Next, I create a DataFrame with 20 sample reviews, ratings, and IDs, and apply the function to clean the review texts.

```{python}
# Creating a sample DataFrame with texts

data = {
    'Review': [
        "Lockdown was necessary for our safety, I fully support it! üòä",
        "I understand the need but it was too long. üòï",
        "Lockdown completely disrupted my life, it was terrible. üò°",
        "It was well-handled and necessary for public health. üè•",
        "Lockdown helped us control the virus spread. üò∑",
        "It was too restrictive and unnecessary for so long. üòí",
        "Not sure if lockdown was the best solution, but we had no choice. ü§∑",
        "It saved lives, but at a great personal cost to many. üòî",
        "The government did what was necessary during the lockdown. üëç",
        "Lockdown was too harsh and not managed well. üò†",
        "It was a good measure, but I hated being stuck at home. üè†üòñ",
        "Lockdown was essential but mental health suffered a lot. üß†üíî",
        "The benefits outweigh the negatives of the lockdown. ‚ûï‚ûñ",
        "Poor execution made it harder than it should be. üòû",
        "Lockdown was over before it was truly safe to do so. üò®",
        "We needed the lockdown to protect vulnerable populations. üõ°Ô∏è",
        "The economic impact of lockdown was devastating. üí∏",
        "Lockdown showed we can take collective action when needed. ü§ù",
        "It was necessary, but the government support was insufficient. üíî",
        "Lockdown should have been stricter to be more effective. ‚ö†Ô∏è"
    ],
    'Sentiment_Rating': [5, 3, 1, 4, 5, 2, 3, 3, 4, 1, 3, 2, 4, 2, 3, 5, 1, 4, 2, 4]
}

# Creating the DataFrame
reviews_df = pd.DataFrame(data)

# Applying the text preprocessing function
reviews_df['Cleaned_Review'] = preprocess_text(reviews_df['Review'].tolist())
pd.set_option('display.max_colwidth', None)
reviews_df
```


